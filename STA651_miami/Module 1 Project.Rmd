---
title: "Module 1 Project"
author: "Rob Carnell"
date: "2025-09-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question

Consider the probability density function

$$f(X|\alpha, \beta) = \alpha \beta x^{-\beta-1} e^{-\alpha x^{-\beta}}$$

for $\alpha \gt 0$, $\beta \gt 0$, $x \gt 0$

The data in the accompanying CSV file were generated from this distribution for some choice of the parameters. Your assignment is to: (1) find the MLE of the parameters, correct to four significant digits; (2) provide 95% confidence intervals on the parameters; and (3) justify the validity of your answers. You are allowed to use any reparameterizationthat you find works.

## Check PDF

$$\int_{0}^{\infty} \alpha \beta x^{-\beta-1} e^{-\alpha x^{-\beta}} dx = e^{-\alpha x^{-\beta}} \big |_{0}^{\infty} = e^{\frac{-\alpha}{x^{\beta}}} \big |_{0}^{\infty} = 1 - 0 = 1$$

Check Numerically:

```{r}
f_pdf <- function(x, a, b) {
  a*b*x^(-b-1)*exp(-a*x^(-b))
}
integrate(f_pdf, 0, Inf, a = 2, b = 3)
```

## Create a CDF and Inverse CDF for sampling

$$F(x) = \int_{0}^{x} \alpha \beta x^{-\beta-1} e^{-\alpha x^{-\beta}} dx = e^{-\alpha x^{-\beta}} \big |_{0}^{x} = e^{-\alpha x^{-\beta}} - 0 =  e^{-\alpha x^{-\beta}}$$

$$F^{-1}(x) = \left(\frac{-1}{\alpha} \log(x)\right)^{\frac{-1}{\beta}}$$

## Find the Expected Value

$$E(X) = \int_0^{\infty} \alpha \beta x^{-\beta} e^{-\alpha x^{-\beta}} dx$$

Reparameterize

$t = \alpha x^{-\beta}$, $dt = -\alpha \beta x^{-beta-1} dx$

$x = \left( \frac{t}{\alpha} \right)^{-1/\beta}$

$x^{-\beta-1} dx = \frac{dt}{-\alpha \beta}$

When $x=0$ then $t=\infty$

When $x=\infty$ then $t=0$

$$E(X) = \int_{\infty}^0 -\left( \frac{t}{\alpha} \right)^{-1/\beta} e^{-t} dt = \int_{0}^{\infty} \alpha^{1/\beta} t^{-1/\beta} e^{-t} dt = \int_{0}^{\infty} \alpha^{1/\beta} t^{(1 -1/\beta) - 1} e^{-t} dt$$

$$=\alpha^{1/\beta} \Gamma(1-\frac{1}{\beta})$$

Where $\beta \le 1$

When $\beta \lt 1$ the integral is divergent


```{r}
F_cdf <- function(x, a, b) {
  exp(-a*x^(-b))
}

F_inv_cdf <- function(p, a, b) {
  ((-1/a)*log(p))^(-1/b)
}

# check
abs(F_cdf(F_inv_cdf(.1, 2, 2), 2, 2) - 0.1) < 1E-9
abs(F_inv_cdf(F_cdf(5, 2, 2), 2, 2) - 5) < 1E-9

expected_f <- function(a, b) {
  integrate(function(x) x*f_pdf(x, a, b), lower = 0, upper = Inf)$value
}

variance_f <- function(a, b) {
  integrate(function(x) x*x*f_pdf(x, a, b), lower = 0, upper = Inf)$value - expected_f(a,b)^2
}

```

```{r}
x_plot <- seq(0, 10, length=1000)
plot(x_plot, f_pdf(x_plot, 2, 3), type = "l", xlab = "x", ylab = "PDF")

plot(x_plot, F_cdf(x_plot, 2, 3), type = "l", xlab = "x", ylab = "CDF")
```

## MLE

Maximizing the likelihood is equivalent to maximizing the log likelihood (because the log is a monotone, increasing function over $x \gt 0$) or minimizing the negative log likelihood.

The likelihood for $N$ independent samples of the random variable $X$:

$$L(X, \alpha, \beta) = \prod_{i=1}^{N} \alpha \beta x_i^{-\beta-1} e^{-\alpha x_i^{-\beta}}$$

$$LL(X, \alpha, \beta) = \sum_{i=1}^N \log(\alpha) + \log(\beta) + (-\beta-1) \log(x_i) - \alpha x_i^{-\beta} = N \log(\alpha) + N \log(\beta) + (-\beta-1) \sum_1^N \log(x_i) - \alpha \sum_1^N x_i^{-\beta}$$

$$nLL(X, \alpha, \beta) = - \sum_{i=1}^N \log(\alpha) + \log(\beta) + (-\beta-1) \log(x_i) - \alpha x_i^{-\beta} = -N \log(\alpha) - N \log(\beta) - (-\beta-1) \sum_1^N \log(x_i) + \alpha \sum_1^N x_i^{-\beta}$$

### Two-Dimensional Optimization

Minimizing the Negative Log Likelihood is equivalent to maximizing the likelihood

$$\frac{\partial nLL}{\partial \alpha} = - \frac{N}{\alpha} + \sum_1^N x_i^{-\beta}$$

$$\frac{\partial^2 LL}{\partial \alpha^2} = \frac{N}{\alpha^2} \gt 0$$

$$\hat{\alpha} = \frac{N}{\sum_1^N x_i^{-\beta}}$$

$$\frac{\partial nLL}{\partial \beta} = - \frac{N}{\beta} + \sum_1^N log(x_i) - \alpha \sum_1^N x_i^{-\beta} log(x_i)$$

$$\frac{\partial^2 nLL}{\partial \beta^2} = \frac{N}{\beta^2} + \alpha \sum_1^N x_i^{-\beta} [log(x_i)]^2 \gt 0$$

$$\frac{\partial nLL}{\partial \alpha \partial \beta} = \frac{\partial nLL}{\partial \beta \partial \alpha} = - \sum_1^N x_i^{-\beta} log(x_i)$$

A simple analytical maximum is not easy to find for $\beta$

### One dimensional Minimization

Substitute the minimum $\hat{\alpha}$ (as a function of $\beta$) into the partial
with respect to $\beta$

$$\frac{d\ nLL}{d\beta} = \frac{d}{d\beta}\left[ -N \log\left(\frac{N}{\sum_1^N x_i^{-\beta}}\right) - N \log(\beta) - (-\beta-1) \sum_1^N \log(x_i) + \left(\frac{N}{\sum_1^N x_i^{-\beta}}\right) \sum_1^N x_i^{-\beta} \right]$$

$$= \frac{d}{d\beta}\left[ -N \log(N) + N \log \left(\frac{N}{\sum_1^N x_i^{-\beta}}\right) - N \log(\beta) - (-\beta-1) \sum_1^N \log(x_i) + N\right]$$

$$= \frac{-N \sum_1^N x_i^{-\beta} \log(x_i)}{\sum_1^N x_i^{-\beta}} - \frac{N}{\beta} + \sum_1^N \log(x_i)$$

## Numerical Solutions

```{r}
# use the negative log likelihood to make optimization easier
nll <- function(x, a, b) {
  N <- length(x)
  -N*log(a) - N*log(b) - (-b-1)*sum(log(x)) + a*sum(x^(-b))
}

jac <- function(x, a, b) {
  N <- length(x)
  partial_a <- -N / a + sum(x^(-b))
  partial_b <- -N / b + sum(log(x)) - a*sum(x^(-b)*log(x))
  return(c(partial_a, partial_b))
}

hess <- function(x, a, b) {
  N <- length(x)
  mat <- matrix(NA, nrow = 2, ncol = 2)
  mat[1,1] <- N/a^2
  mat[2,1] <- -1*sum(x^-b*log(x))
  mat[1,2] <- -1*sum(x^-b*log(x))
  mat[2,2] <- N/b^2 + a*sum(x^-b*(log(x))^2)
  return(mat)
}
```

Take a sample for demonstration

```{r}
set.seed(1928)
X_sample <- F_inv_cdf(runif(1000), 2, 3)
```

### Optimize using `stats4::mle`

```{r}
o1 <- stats4::mle(minuslogl = function(a, b) nll(X_sample, a, b), 
                  start = list(a = 1, b = 1),
                  lower = list(a = 1E-9, b = 1E-9))

o1

# check that the alpha-hat equation works
length(X_sample)/sum(X_sample^-o1@coef[2])

# check that the \beta partial derivative is minimized at beta-hat
optimize(f = function(b) {(jac(X_sample, o1@coef[1], b)[2])^2},
         lower = 0, upper = 20)$minimum
```

Create the numerical confidence intervals

```{r}
profile1 <- stats4::profile(o1)

stats4::confint(profile1)

# check that the confidence intervals are normal 95% confidence intervals
c(o1@coef[1] + qnorm(0.025)*sqrt(o1@vcov[1,1]),
  o1@coef[1] + qnorm(0.975)*sqrt(o1@vcov[1,1]))

c(o1@coef[2] + qnorm(0.025)*sqrt(o1@vcov[2,2]),
  o1@coef[2] + qnorm(0.975)*sqrt(o1@vcov[2,2]))

stats4::plot(profile1)
```

### Optimize using `optim`

```{r}
o2 <- optim(c(1,1), function(par) {nll(X_sample, par[1], par[2])}, hessian = TRUE)

o2

# check that the alpha-hat equation works
length(X_sample)/sum(X_sample^-o2$par[2])

# check that the \beta partial derivative is minimized at beta-hat
optimize(f = function(b) {(jac(X_sample, o2$par[1], b)[2])^2},
         lower = 0, upper = 20)$minimum
```

Create the numerical confidence intervals

```{r}
v <- solve(o2$hessian)

c(o2$par[1] + qnorm(0.025)*sqrt(v[1,1]),
  o2$par[1] + qnorm(0.975)*sqrt(v[1,1]))

c(o2$par[2] + qnorm(0.025)*sqrt(v[2,2]),
  o2$par[2] + qnorm(0.975)*sqrt(v[2,2]))
```

Check Hessian

```{r}
# analytical
hess(X_sample, o2$par[1], o2$par[2])
# 2-D optimization - empirical estimate
o2$hessian
# Inverse of the Hessian is the VCOV
solve(hess(X_sample, o2$par[1], o2$par[2]))
# Empirical VCOV from the MLE function
o1@vcov
```

Check moments

```{r}
mean(X_sample)
expected_f(o1@coef[1], o1@coef[2])

var(X_sample)
variance_f(o1@coef[1], o1@coef[2])
```

### 1-D Optimize using `optimize`

```{r}
o1d <- optimize(function(b) {
  N <- length(X_sample)
  (-1*N*sum(X_sample^(-b)*log(X_sample)) / sum(X_sample^(-b)) -N/b + sum(log(X_sample)))^2
}, lower = 1.1, upper = 10, maximum = FALSE)

# alpha
length(X_sample) / sum(X_sample^(-o1d$minimum))

# beta
o1d$minimum
```

## unknown sample

```{r}
X_final_sample <- unlist(read.csv("projectdata-DarbyS.csv"))
```

```{r}
o3 <- stats4::mle(minuslogl = function(a, b) nll(X_final_sample, a, b), 
                  start = list(a = 1, b = 1),
                  lower = list(a = 1E-9, b = 1E-9))

o3

profile3 <- stats4::profile(o3)

stats4::confint(profile3)

stats4::plot(profile3)
```

```{r}
# bootstrap confidence intervals for the MLE
b <- boot::boot(X_final_sample, statistic = function(d, i) {
  temp <- stats4::mle(minuslogl = function(a, b) nll(d[i], a, b), 
                  start = list(a = 1, b = 1),
                  lower = list(a = 1E-9, b = 1E-9))
  return(temp@coef)
}, R = 500)

boot::boot.ci(b, type = "perc", index = 1)
boot::boot.ci(b, type = "perc", index = 2)

stats4::confint(profile3)

```

```{r}
# check moments
#   Since beta is less than 1, the mean does not exist

# check density
plot(density(log(X_final_sample), bw = "SJ"), main = "Kernel density vs MLE density")
lines(seq(-4, 10, length=100), f_pdf(exp(seq(-4, 10, length=100)), o3@coef[1], o3@coef[2]), col = "red", lty = 2)

```
