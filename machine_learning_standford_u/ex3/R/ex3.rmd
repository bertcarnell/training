---
title: "Coursera Machine Learning Exercise 3"
author: "Rob Carnell"
date: "January 29, 2018"
output: html_document
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
# plotting
require(R.matlab)
require(ggplot2)

user <- Sys.info()["user"]
repositoryPath <- NA
if (tolower(user) == "hb82795")
{
  repositoryPath <- file.path("C:", "developer","repositories","DataAnalyticsTeam")
}
stopifnot(file.exists(repositoryPath))

dataPath <- file.path(repositoryPath, "Training", "Coursera_Machine_Learning", 
                      "machine-learning-ex3", "data")
```

# Multi-class logistic regression

## Read in the Data

```{r}
ex3data <- readMat(file.path(dataPath, "ex3data1.mat"))
```

## Plot the numerals

```{r}
set.seed(1976)
df1 <- data.frame(x=rep(seq(0.5,19.5), each=20), y=rep(seq(-0.5,-19.5), times=20), 
                  grouprow=rep(1:10, each=400, times=10),
                  groupcol=rep(1:10, each=4000), 
                  value=c(t(ex3data$X[sample(1:5000, 100),])))
g1 <- ggplot(df1, aes(x =x, y = y, fill = value)) + 
  geom_raster() + facet_grid(grouprow ~ groupcol) +
  scale_fill_continuous(high = "white", low = "black", guide = "none") +
  theme_bw(base_size = 14) +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank(),
        strip.text.y = element_blank())
plot(g1)

```

# Functions

```{r}
bindOnes <- function(X)
{
  m <- nrow(X)
  return(matrix(c(rep(1,m), X), nrow=m))
}
stopifnot(all(c(bindOnes(matrix(c(1,2,3,4,5,6),nrow=3))) == c(1,1,1,1,2,3,4,5,6)))

createColumnVec <- function(z)
{
  return(matrix(z, ncol=1))
}

optimComputeCostRegularization <- function(theta, X, y, lambda)
{
  theta <- matrix(theta, ncol=1)
  y <- matrix(y, ncol=1)
  m <- nrow(y)
  temp <- X %*% theta
  log_h_theta <- plogis(temp, log.p = TRUE)
  log_1_minus_h_theta <- log(1-plogis(temp))
  # for regularization, theta_0 is not used, so set it to zero
  theta[1,1] <- 0
  J <- 1 / m * sum(-1*y*log_h_theta - (1-y)*log_1_minus_h_theta) + 
    lambda / 2 / m * sum(theta^2)
  J <- ifelse(is.na(J), .Machine$double.xmax, J)
  return(J)
}

optimCostGradientRegularization <- function(theta, X, y, lambda)
{
  theta <- matrix(theta, nrow=ncol(X))
  y <- matrix(y, ncol=1)
  m <- nrow(y)
  temp <- X %*% theta
  # for regularization, theta_0 is not used, so set it to zero
  theta[1,1] <- 0
  dJ <- 1 / m * t(X) %*% (plogis(temp) - y) + lambda / m * theta
  return(dJ)
}

oneVsAll <- function(X, y, num_labels, lambda)
{
  n <- ncol(X)
  all_theta <- matrix(0, nrow = num_labels, ncol = n + 1)
  X <- bindOnes(X)
  
  for (i in 1:num_labels)
  {
    temp_test <- ifelse(y == i, 1, 0)
    o <- optim(rep(0.1, n + 1), optimComputeCostRegularization, 
               gr = optimCostGradientRegularization,
               X = X, y = temp_test, lambda = lambda, method = "BFGS")
    stopifnot(o$converence == 0)
    if (o$value == .Machine$double.xmax)
      warning("Change starting values for the optim procedure")
    all_theta[i,] <- o$par
  }
  return(all_theta)
}

theta_t <- c(-2, -1, 1, 2);
X_t <- matrix(c(rep(1,5), (1:15)/10), nrow=5, ncol=4)
y_t <- createColumnVec(c(1,0,1,0,1))
lambda_t <- 3
J <- optimComputeCostRegularization(theta_t, X_t, y_t, lambda_t)
stopifnot(abs(J-2.534819) < 0.001)
gr <- optimCostGradientRegularization(theta_t, X_t, y_t, lambda_t)
stopifnot(all(abs(gr - c(0.146561, -0.548558, 0.724722, 1.398003)) < 0.001))

predictOneVsAll <- function(all_theta, X)
{
  X <- bindOnes(X)
  # m x n+1 * k x n+1
  probs <- plogis(X %*% t(all_theta))
  class <- apply(probs, 1, which.max)
  return(list(probs=probs, class=class))
}

```

# One vs All Regression

```{r}
X <- as.matrix(ex3data$X)
y <- createColumnVec(ex3data$y)
num_labels <- 10
stopifnot(length(unique(y)) == num_labels)
lambda <- 0.1
all_theta <- oneVsAll(X, y, num_labels, lambda);

```

## One vs. all prediction

```{r}
pred <- predictOneVsAll(all_theta, X)

confusionMatrix <- table(pred$class, y)
dimnames(confusionMatrix) <- list(predicted=1:10,
                                  actual=1:10)

confusionMatrix

accuracy <- mean(ifelse(pred$class == y, 1, 0))

stopifnot(abs(accuracy - 0.949) < 0.01)

```

# Neural Network Calculations

## Read the data
```{r}
ex3weights <- readMat(file.path(dataPath, "ex3weights.mat"))
dim(ex3weights$Theta1)
dim(ex3weights$Theta2)

```

## Predict

```{r}
predictNeuralNetwork <- function(theta1, theta2, X)
{
  X <- bindOnes(X)
  # m x n+1 * (25 x n+1)' 
  z2 <- X %*% t(theta1)
  # m x 25
  a2 <- plogis(z2);
  # m x 25+1
  a2 <- bindOnes(a2)
  # m x 25+1 * (10 x 26)'
  z3 <- a2 %*% t(theta2)
  a3 <- plogis(z3)
  probs <- a3
  classes <- apply(a3, 1, which.max)
  return(list(probs=probs, classes=classes))
}

res <- predictNeuralNetwork(as.matrix(ex3weights$Theta1), 
                            as.matrix(ex3weights$Theta2), 
                            as.matrix(ex3data$X))

confusionMatrix <- table(res$classes, ex3data$y)
dimnames(confusionMatrix) <- list(predicted=1:10,
                                  actual=1:10)
confusionMatrix

accuracy <- mean(ifelse(res$classes == ex3data$y, 1, 0))

stopifnot(abs(accuracy - 0.975) < 0.01)

```
